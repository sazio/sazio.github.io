---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

**Key Interests: Representation Learning, Biological Inspiration & Inductive Bias, Equivariant Representations, Vision**

Salut! I am currently a Research Engineer in [Brice Bathellier](https://www.bathellier-lab.org/people/Brice)'s Lab at the [Hearing Institute](https://www.institut-audition.fr/en) (Institut Pasteur) in Paris, working on nonlinear network models for capturing the transformations that occur across the different stages of the auditory system. Previously, I worked as a Research Intern at CERN, under the supervision of [Lorenzo Moneta](https://root.cern/about/team/#Lorenzo%20Moneta), developing data conversion and visualization techniques for the ROOT framework. Before that, I worked on nonlinear models of sequential spatial memory in [Andr√© Longtin](https://mysite.science.uottawa.ca/alongtin/)'s  Neurophysics and Nonlinear Dynamics group, at the University of Ottawa, co-supervised by [Leonard Maler](https://www.uottawa.ca/brain/people/maler-leonard) and [Lamberto Rondoni](http://calvino.polito.it/~rondoni/). In the meantime, I completed a M.Sc. in Physics of Complex Systems at the University of Turin. 


I'm also a co-founder and president of the [Machine Learning Journal Club (MLJC)](https://www.mljc.it) a non-profit research organization, focused on interdisciplinary applications of Machine and Deep Learning, with an [open-source attitude](https://github.com/MachineLearningJournalClub). 

Through MLJC, I worked and I'm currently working on the following topics: 
* **Scientific Machine Learning**: Physics Informed Neural Networks, with applications on [Wildfire Spreading](http://ceur-ws.org/Vol-2964/article_171.pdf); Quadrature Methods for [NeuralPDE](https://neuralpde.sciml.ai/stable/), wrote a [preprint](https://arxiv.org/abs/2107.09443) by collaborating with Julia Computing and [Christopher V. Rackauckas](https://chrisrackauckas.com/) from MIT Julia Lab; Now working on Nonlinear Dynamical Systems identification and control for Real Time Machine Learning with FPGAs.
* **Brain Computer Interfaces**: Topological Data Analysis approach for extracting relevant features on ECoG's data ([preprint](https://arxiv.org/abs/2110.04653), [code](https://github.com/MachineLearningJournalClub/ECoG_VBH_2021)); Random Convolutional Kernels for Near Real Time feature extraction from SSVEP data
* **Natural Language Processing**: Designed an introductory course on [NLP](https://github.com/MachineLearningJournalClub/LearningNLP); Worked on Sentence Transformers models for Readability; Took part in [HuggingFace x JAX community week](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354) by working on the largest sentence transformer model (~1B params)

Recently, I'm organizing a new group on deepening the role of **Symmetries** in the learning process, we are currently exploring:

* What could be the relationship between **inductive biases** in Geometric Deep Learning and Physics Informed Machine Learning models
* Symmetry regularization as a biologically plausible form of regularization
* [Brainscore 2022 Competition](http://www.brain-score.org/competition/): is group sparsity (pinwheels in V1) another way of imposing topographic properties in CNN layers? 

## Research Interests


I‚Äôm truly fascinated by **representation learning**, in particular by the idea of uncovering the underlying principles and symmetries, i.e. informative low dimensional geometrical structures, that allow for invariant and stable representations. 

I believe that there can be different standpoints on this very general problem, for example from a Physicist's perspective: using physics as a ‚Äúregularizer‚Äù for learning with artificial neural nets is related to learning meaningful low dimensional representations. This recalls Noether‚Äôs theorem in Physics: it is possible to determine conserved quantities, i.e. invariants, from the observed symmetries of a physical system. Similarly, it is possible to consider classes of Lagrangians with given invariants to describe a physical system. Thus, I believe there is a strict relationship between understanding invariant representations by imposing low-level structures and hypothesizing specific physical laws which turn out to give place to symmetrical structures.

While being interested in the artificial perspective on neural networks, I‚Äôm convinced that if we want to advance science, by following Feynman‚Äôs formula *‚Äúwhat I cannot create, I do not understand‚Äù*, we need to consider biological learning. To characterize biological intelligence as a complex system, I aim to examine the transition between micro, meso and macro scales. As anticipated by **David Marr**, to design a meaningful neural network theory and make testable and valuable predictions, we need to tie it to neural circuitry. I consider Marr‚Äôs framework as the backbone of a complicated but feasible process: with a radical extension, i.e. coupling and studying simultaneously the different levels of organization of the brain, we might be able to improve our current understanding.

Feel free to [contact me](mailto:simone.azeglio@pasteur.fr) if you are interested in one of MLJC's projects or if we share some interests!


## News

[NeurReps Workshop (NeurIPS 2022)](https://www.neurreps.org/)
---------

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Glad to be part of the organizing committee of this <a href="https://twitter.com/NeurIPSConf?ref_src=twsrc%5Etfw">@NeurIPSConf</a> workshop! <br><br>Further updates as well as a call for papers are coming soon üëÄ<br><br>With <a href="https://twitter.com/naturecomputes?ref_src=twsrc%5Etfw">@naturecomputes</a> <a href="https://twitter.com/cashewmake2?ref_src=twsrc%5Etfw">@cashewmake2</a> <a href="https://twitter.com/ari_dibe?ref_src=twsrc%5Etfw">@ari_dibe</a> <a href="https://twitter.com/ninamiolane?ref_src=twsrc%5Etfw">@ninamiolane</a> <a href="https://t.co/VmLEG4DIf7">https://t.co/VmLEG4DIf7</a></p>&mdash; Simone Azeglio (@simoneazeglio) <a href="https://twitter.com/simoneazeglio/status/1545325709840760835?ref_src=twsrc%5Etfw">July 8, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---------

[Brains for Brains Award 2022](https://bernstein-network.de/en/newsroom/news/brains-for-brains-awardee-2022/)
---------
Since 2010, the Bernstein Network Computational Neuroscience presents the Brains for Brains Award, which recognizes the special achievements of young scientists who have shown their outstanding potential at a very early career stage ‚Äì even before starting their doctoral studies. It is one of two prizes awarded bi-yearly by the Bernstein Network. This award aims to attract young international researchers to Germany.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Deeply pleased and honored to receive this award ! Can&#39;t wait to present our work at the <a href="https://twitter.com/hashtag/BernsteinConference?src=hash&amp;ref_src=twsrc%5Etfw">#BernsteinConference</a>. <br>Thanks <a href="https://twitter.com/BathellierL?ref_src=twsrc%5Etfw">@BathellierL</a>, <a href="https://twitter.com/institutpasteur?ref_src=twsrc%5Etfw">@institutpasteur</a> for your support üß† <a href="https://t.co/7ofJXFNkDp">https://t.co/7ofJXFNkDp</a></p>&mdash; Simone Azeglio (@simoneazeglio) <a href="https://twitter.com/simoneazeglio/status/1536414586190143489?ref_src=twsrc%5Etfw">June 13, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

----------

[Symmetry, Invariance and Neural Representations](https://bernstein-network.de/bernstein-conference/program/satellite-workshops/symmetry-invariance-and-neural-representations/)
---------
Azeglio & Di Bernardo, <i>Symmetry, Invariance and Neural Representations</i> Bernstein Conference 2022

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">It&#39;s official! <a href="https://twitter.com/BernsteinNeuro?ref_src=twsrc%5Etfw">@BernsteinNeuro</a> accepted our - with <a href="https://twitter.com/ari_dibe?ref_src=twsrc%5Etfw">@ari_dibe</a> - workshop proposal üß† <br><br>In &quot;Symmetry, Invariance and Neural Representations&quot; we will explore the intimate relationship between the physical world and neural representations. <br><br>Join our speakers in this experience! <a href="https://t.co/eOisKkGGum">pic.twitter.com/eOisKkGGum</a></p>&mdash; Simone Azeglio (@simoneazeglio) <a href="https://twitter.com/simoneazeglio/status/1523036342565105664?ref_src=twsrc%5Etfw">May 7, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

----------


[Improving Neural Predictivity in the Visual Cortex with Gated Recurrent Connections](https://openreview.net/references/pdf?id=HbNa-jRWf5)
---------
Azeglio, Poetto, Savant-Aira, Nurisso, <i>Improving Neural Predictivity in the Visual Cortex with Gated Recurrent Connections </i> Brainscore Workshop - Cosyne 2022

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/cosyne2022?src=hash&amp;ref_src=twsrc%5Etfw">#cosyne2022</a> was awesome! I had the pleasure to present our work at the <a href="https://twitter.com/brain_score?ref_src=twsrc%5Etfw">@brain_score</a> workshop(<a href="https://t.co/VFa7r7YvZy">https://t.co/VFa7r7YvZy</a>), as part of the selected models submitted to the Brain-Score competition. TLDR: We employed a Gated Recurrent Convolutional Network w/ some fancy augmentation <a href="https://t.co/Iv6sSdpSRo">pic.twitter.com/Iv6sSdpSRo</a></p>&mdash; Simone Azeglio (@simoneazeglio) <a href="https://twitter.com/simoneazeglio/status/1507419565013282821?ref_src=twsrc%5Etfw">March 25, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

