---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

**Key Interests: Representation Learning, Biological Inspiration & Inductive Bias, Equivariant Representations**

Salut! I am currently a Research Engineer in [Brice Bathellier](https://www.bathellier-lab.org/people/Brice)'s Lab at the [Hearing Institute](https://www.institut-audition.fr/en) in Paris, working on nonlinear network models for capturing the transformations that occur across the different stages of the auditory system. Previously, I worked as a Research Intern at CERN, under the supervision of [Lorenzo Moneta](https://root.cern/about/team/#Lorenzo%20Moneta), developing data conversion and visualization techniques for the ROOT framework. Before that, I worked on nonlinear models of sequential spatial memory in [André Longtin](https://mysite.science.uottawa.ca/alongtin/)'s  Neurophysics and Nonlinear Dynamics group, at the University of Ottawa, co-supervised by [Leonard Maler](https://www.uottawa.ca/brain/people/maler-leonard) and [Lamberto Rondoni](http://calvino.polito.it/~rondoni/). In the meantime, I completed a M.Sc. in Physics of Complex Systems at the University of Turin. 


I'm also a co-founder and president of the [Machine Learning Journal Club (MLJC)](https://www.mljc.it) a non-profit research organization, focused on interdisciplinary applications of Machine and Deep Learning, with an [open-source attitude](https://github.com/MachineLearningJournalClub). 

Through MLJC, I worked and I'm currently working on the following topics: 
* **Scientific Machine Learning**: Physics Informed Neural Networks, with applications on [Wildfire Spreading](http://ceur-ws.org/Vol-2964/article_171.pdf); Quadrature Methods for [NeuralPDE](https://neuralpde.sciml.ai/stable/), wrote a [preprint](https://arxiv.org/abs/2107.09443) by collaborating with Julia Computing and [Christopher V. Rackauckas](https://chrisrackauckas.com/) from MIT Julia Lab; Now working on Nonlinear Dynamical Systems identification and control for Real Time Machine Learning with FPGAs.
* **Brain Computer Interfaces**: Topological Data Analysis approach for extracting relevant features on ECoG's data ([preprint](https://arxiv.org/abs/2110.04653), [code](https://github.com/MachineLearningJournalClub/ECoG_VBH_2021)); Random Convolutional Kernels for Near Real Time feature extraction from SSVEP data
* **Natural Language Processing**: Designed an introductory course on [NLP](https://github.com/MachineLearningJournalClub/LearningNLP); Worked on Sentence Transformers models for Readability; Took part in [HuggingFace x JAX community week](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354) by working on the largest sentence transformer model (~1B params)

Recently, I'm organizing a new group on deepening the role of **Symmetries** in the learning process, we are currently exploring:

* What could be the relationship between **inductive biases** in Geometric Deep Learning and Physics Informed Machine Learning models
* Symmetry regularization as a biologically plausible form of regularization
* [Brainscore 2022 Competition](http://www.brain-score.org/competition/): is group sparsity (pinwheels in V1) another way of imposing topographic properties in CNN layers? 


## Research Interests


I’m truly fascinated by **representation learning**, in particular by the idea of uncovering the underlying principles and symmetries, i.e. informative low dimensional geometrical structures, that allow for invariant and stable representations. 

I believe that there can be different standpoints on this very general problem, for example from a Physicist's perspective: using physics as a “regularizer” for learning with artificial neural nets is related to learning meaningful low dimensional representations. This recalls Noether’s theorem in Physics: it is possible to determine conserved quantities, i.e. invariants, from the observed symmetries of a physical system. Similarly, it is possible to consider classes of Lagrangians with given invariants to describe a physical system. Thus, I believe there is a strict relationship between understanding invariant representations by imposing low-level structures and hypothesizing specific physical laws which turn out to give place to symmetrical structures.

While being interested in the artificial perspective on neural networks, I’m convinced that if we want to advance science, by following Feynman’s formula *“what I cannot create, I do not understand”*, we need to consider biological learning. To characterize biological intelligence as a complex system, I aim to examine the transition between micro, meso and macro scales. As anticipated by **David Marr**, to design a meaningful neural network theory and make testable and valuable predictions, we need to tie it to neural circuitry. I consider Marr’s framework as the backbone of a complicated but feasible process: with a radical extension, i.e. coupling and studying simultaneously the different levels of organization of the brain, we might be able to improve our current understanding.

Feel free to [contact me](mailto:simone.azeglio@pasteur.fr) if you are interested in one of MLJC's projects or if we share some interests!


--- 


