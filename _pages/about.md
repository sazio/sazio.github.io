---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

**Key Interests: Representation Learning, NeuroAI, Equivariant Representations**

Salut! I am currently a Research Engineer in [Brice Bathellier](https://www.bathellier-lab.org/people/Brice)'s Lab at the Hearing Institute in Paris, working on nonlinear network models for capturing the transformations that occurs accross the different stages of the auditory system. Previously, I worked as a Research Intern at CERN, under the supervision of [Lorenzo Moneta](https://root.cern/about/team/#Lorenzo%20Moneta), developing data conversion and visualization for the ROOT framework. Before that, I worked on nonlinear models of sequential spatial memory in [André Longtin](https://mysite.science.uottawa.ca/alongtin/)'s  Neurophysics and Nonlinear Dynamics group, at the University of Ottawa, co-supervised by [Leonard Maler](https://www.uottawa.ca/brain/people/maler-leonard) and [Lamberto Rondoni](http://calvino.polito.it/~rondoni/). In the meantime, I completed a M.Sc. in Physics of Complex Systems at the University of Turin. 


I'm also the co-founder and president of the [Machine Learning Journal Club (MLJC)](https://www.mljc.it) a non-profit research organization, focused on interdisciplinary applications of Machine and Deep Learning, with an [open-source attitude](https://github.com/MachineLearningJournalClub). 

## Research Interests


I’m truly fascinated by **representation learning**, in particular by the idea of uncovering the underlying principles and symmetries, i.e. informative low dimensional geometrical structures, that allow for invariant and stable representations. 

I believe that there can be different standpoints on this very general problem, for example from a Physicist's perspective: using physics as a “regularizer” for learning with artificial neural nets is related to learning meaningful low dimensional representations. This recalls Noether’s theorem in Physics: it is possible to determine conserved quantities, i.e. invariants, from the observed symmetries of a physical system. Similarly, it is possible to consider classes of Lagrangians with given invariants to describe a physical system. Thus, I believe there is a strict relationship between understanding invariant representations by imposing low-level structures and hypothesizing specific physical laws which turn out to give place to symmetrical structures.

While being interested in the artificial perspective on neural networks, I’m convinced that if we want to advance science, by following Feynman’s formula *“what I cannot create, I do not understand”*, we need to consider biological learning. To characterize biological intelligence as a complex system, I aim to examine the transition between micro, meso and macro scales. As anticipated by **David Marr**, to design a meaningful neural network theory and make testable and valuable predictions, we need to tie it to neural circuitry. I consider Marr’s framework as the backbone of a complicated but feasible process: with a radical extension, i.e. coupling and studying simultaneously the different levels of organization of the brain, we might be able to improve our current understanding.


--- 


